#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{fancyhdr} % activamos el paquete
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}
{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}


\pagestyle{fancy} % seleccionamos un estilo
%\lhead{TEXTO LEFT} % texto izquierda de la cabecera
%\chead{TEXTO CENTER} % texto centro de la cabecera
%\rhead{\thepage} % número de página a la derecha
%\lfoot{TEXTO LEFT} % texto izquierda del pie
%\cfoot{\includegraphics[width=11cm]{logo}} % imagen centro del pie
\cfoot{Deiner Zapata S.}
\rfoot{\thepage} % texto derecha del pie
\renewcommand{\headrulewidth}{0.4pt} % grosor de la línea de la cabecera
\renewcommand{\footrulewidth}{0.4pt} % grosor de la línea del pie
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language spanish-mexico
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_author "Deiner Lalix Zapata"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 4cm
\headheight 2cm
\headsep 2cm
\footskip 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning
\end_layout

\begin_layout Section
Regresión Lineal Simple
\end_layout

\begin_layout Standard
En Scikit-Learn cada clase del modelo es representado por una clase Python.
 Si queremos calcular una regresió lineal simple, nosotros importamos la
 clase 
\begin_inset Quotes eld
\end_inset

Regresión Linear
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.linear_model import LinearRegression
\end_layout

\begin_layout Plain Layout

import matplotlib.pyplot as plt
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#Semilla para generar los mismos numeros aleatorios
\end_layout

\begin_layout Plain Layout

rng = np.random.RandomState(42)
\end_layout

\begin_layout Plain Layout

x = 10 * rng.rand(50)
\end_layout

\begin_layout Plain Layout

y = 2 * x - 1 + rng.randn(50)
\end_layout

\begin_layout Plain Layout

plt.scatter
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generación de datos
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
La ecuación que se trata de encontrar es:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f\left(x\right):=y=2\cdot x+1
\]

\end_inset


\end_layout

\begin_layout Standard
Claramente tiene una intersección en el eje y=1, por tal motivo usamos 
\begin_inset Quotes eld
\end_inset

fit_intercept=True
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# Instanciando un objeto modelo de la clase 
\begin_inset Quotes eld
\end_inset

LinealRegression
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

model =LinealRegression(fit_intercept=True)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Convirtiendo los ventores en matrices
\end_layout

\begin_layout Plain Layout

X=x[: , np.newaxis]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Entrenando el modelo con los datos
\end_layout

\begin_layout Plain Layout

model.fit(X,y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Obteniendo los coeficientes y = A*x + B
\end_layout

\begin_layout Plain Layout

A = model.coef_
\end_layout

\begin_layout Plain Layout

B = model.intercept_
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Entrenando el modelo lineal
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Después de tener el modelo lineal entrenado, se procederá a predecir nuevos
 valores usando el modelo previamente entrenado, tal como se muestra acontinuaci
ón:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# Generando nuevos datos
\end_layout

\begin_layout Plain Layout

xfit = np.linspace(-1,11)
\end_layout

\begin_layout Plain Layout

# Convirtiendo los datos en matriz
\end_layout

\begin_layout Plain Layout

Xfit = xfit[:, np.newaxis]
\end_layout

\begin_layout Plain Layout

# Calculando los nuevos valores con el modelo entrenado
\end_layout

\begin_layout Plain Layout

yfit = model.predict(Xfit)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Prediciendo nuevos valores con el modelo lineal
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Regresión Polinomial
\end_layout

\begin_layout Standard
Tomando un modelo multidimencional lineal de la forma:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=a_{0}+a_{1}\cdot x_{1}+a_{2}\cdot x_{2}+a_{3}\cdot x_{3}+\ldots
\]

\end_inset


\end_layout

\begin_layout Standard
Donde los coeficientes 
\begin_inset Formula $x_{n}$
\end_inset

 están determinados por una función que transforma los datos 
\begin_inset Formula $f_{n}\left(x\right)=x^{n}$
\end_inset

, nuestro modelo se convierte en una regresión polinomial:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=a_{0}+a_{1}\cdot x+a_{2}\cdot x^{2}+a_{3}\cdot x^{3}+\ldots
\]

\end_inset


\end_layout

\begin_layout Standard
Denotando que esto es todavía un modelo lineal, haciendo referente a encontrar
 los coeficientes 
\begin_inset Formula $a_{n}$
\end_inset

.
\end_layout

\begin_layout Subsection
Funciones Polinomiales Básicas
\end_layout

\begin_layout Standard
Esta proyección polinomial es muy util y Scikit-Learn viene integrada con
 una funcionalidad para cacular los 
\begin_inset Formula $x^{n}$
\end_inset

, para ello se usara 
\series bold
PolinomialFeatures 
\series default
transformer:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.preprocessing import PolynomialFeatures
\end_layout

\begin_layout Plain Layout

x = np.array([2, 3, 4])
\end_layout

\begin_layout Plain Layout

poly = PolynomialFeatures(3, include_bias=False)
\end_layout

\begin_layout Plain Layout

poly.fit_transform(x[:,None])
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Usando 
\begin_inset Quotes eld
\end_inset

PolinomialFeatures
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La salida mostrada en pantalla será 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

array([[ 2.,  4.,  8.],
\end_layout

\begin_layout Plain Layout

       [ 3.,  9., 27.],
\end_layout

\begin_layout Plain Layout

       [ 4., 16., 64.]])
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Esta transformación a convertido un arreglo unidimensional en un arreglo
 tridimensional, con este nuevo arreglo multidimensional, podemos realizar
 una regresión lineal.
 La mejor forma de hacer esto es usar un pipeline.
\end_layout

\begin_layout Standard
Haciendo que el modelo polinomial tenga 7 grados:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.pipeline import make_pipeline
\end_layout

\begin_layout Plain Layout

degree = 7
\end_layout

\begin_layout Plain Layout

poly_model = make_pipeline( PolynomialFeatures(degree), LinealRegression()
 )
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Instanciando modelo polinomial de 7 grados
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Con esta transformación, podemos construir un modelo mucho mas complejo
 que relacione 
\begin_inset Quotes eld
\end_inset

x
\begin_inset Quotes erd
\end_inset

 e 
\begin_inset Quotes eld
\end_inset

y
\begin_inset Quotes erd
\end_inset

.
 Por ejemplo, modelar una onda seno con ruido 
\begin_inset Formula $\xi$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{real}=\sin\left(x_{real}\right)+\xi
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

rng = np.random.RandomState(1)
\end_layout

\begin_layout Plain Layout

# Generando 50 datos aleatorios desde 0 a 10.
\end_layout

\begin_layout Plain Layout

x = 10 * rng.rand(50)
\end_layout

\begin_layout Plain Layout

y = np.sin(x) + 0.1 * rng.randn(50)
\end_layout

\begin_layout Plain Layout

poly_model.fit(x[:, np.newaxis], y)
\end_layout

\begin_layout Plain Layout

xfit = np.linspace(0, 10, 1000)
\end_layout

\begin_layout Plain Layout

yfit = poly_model.predict( xfit[:, np.newaxis])
\end_layout

\begin_layout Plain Layout

plt.scatter(x,y)
\end_layout

\begin_layout Plain Layout

plt.plot(xfit, yfit)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Modelado polinomial de una función senoidal
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Regresión con Funciones Básicas Gausianas
\end_layout

\begin_layout Standard
Otra forma de modelar los datos es usar una suma de funciones Gausianas
 como base.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename CompositionFromGaussianFunction.png
	scale 80
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Modelado a partir de funciones Gausianas
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Las funciones Gaussianas basicas no estan construidas en Scikit-Learn, poro
 podemos escribir una transformador que los crea, tal como se muestra a
 continuación:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.base import BaseEstimator, TransformerMixin
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

class GaussianFeatures(BaseEstimator, TransformerMixin):
\end_layout

\begin_layout Plain Layout

    """Uniformly spaced Gaussian features for one-dimensional input"""
\end_layout

\begin_layout Plain Layout

    def __init__(self, N, width_factor=2.0):
\end_layout

\begin_layout Plain Layout

        self.N = N
\end_layout

\begin_layout Plain Layout

        self.width_factor = width_factor
\end_layout

\begin_layout Plain Layout

         @staticmethod
\end_layout

\begin_layout Plain Layout

    def _gauss_basis(x, y, width, axis=None):
\end_layout

\begin_layout Plain Layout

        arg = (x - y) / width
\end_layout

\begin_layout Plain Layout

        return np.exp(-0.5 * np.sum(arg ** 2, axis))
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    def fit(self, X, y=None):
\end_layout

\begin_layout Plain Layout

        # create N centers spread along the data range
\end_layout

\begin_layout Plain Layout

        self.centers_ = np.linspace(X.min(), X.max(), self.N)
\end_layout

\begin_layout Plain Layout

        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])
\end_layout

\begin_layout Plain Layout

        return self
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    def transform(self, X):
\end_layout

\begin_layout Plain Layout

        return self._gauss_basis(X[:, :, np.newaxis],
\end_layout

\begin_layout Plain Layout

                                 self.centers_,
\end_layout

\begin_layout Plain Layout

                                 self.width_, axis=1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

gauss_model = make_pipeline(GaussianFeatures(20),LinearRegression())
\end_layout

\begin_layout Plain Layout

gauss_model.fit(x[:, np.newaxis], y)
\end_layout

\begin_layout Plain Layout

yfit = gauss_model.predict(xfit[:, np.newaxis])
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.scatter(x, y)
\end_layout

\begin_layout Plain Layout

plt.plot(xfit, yfit)
\end_layout

\begin_layout Plain Layout

plt.xlim(0, 10)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clase Gausiana
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A continuación se muestra el modelo ajustado con los datos de entrada, como
 se podra observar, no se ajusta mucho a una onda senoidal.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitSenoidalWithGaussFunctions.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Modelado con funciones Gausianas 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Regresión Ridge o Regularización 
\begin_inset Formula $L_{2}$
\end_inset


\end_layout

\begin_layout Standard
También llamada Tikhonov regularización.
 Este tipo de regresión usa la penalización de la suma de cuadrados (2-norms)
 del modelo de coeficientes, en este caso el modelo es:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P=\alpha\cdot\sum_{n=1}^{N}\theta_{n}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Donde 
\begin_inset Formula $\alpha$
\end_inset

 es un parametro libre que controla la fuerza de la penalidad, este modelo
 ya viene implementado en Scikit-Learn con 
\begin_inset Quotes fld
\end_inset

Ridge
\begin_inset Quotes frd
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.lineal_model import Ridge
\end_layout

\begin_layout Plain Layout

model = make_pipeline(GaussianFeatures(30), Ridge(aplha=0.1))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fig, ax = plt.subplots(2, sharex=True)
\end_layout

\begin_layout Plain Layout

model.fit(x[:,np.newaxis],y)
\end_layout

\begin_layout Plain Layout

ax[0].scatter(x,y)
\end_layout

\begin_layout Plain Layout

ax[0].plot(xfit, model.predict(xfit[:,np.newaxis]))
\end_layout

\begin_layout Plain Layout

ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5,1.5))
\end_layout

\begin_layout Plain Layout

ax[0].set_title(title)
\end_layout

\begin_layout Plain Layout

ax[1].plot(model.steps[0][1].centers_,
\end_layout

\begin_layout Plain Layout

           model.steps[1][1].coef_)
\end_layout

\begin_layout Plain Layout

ax[1].set(xlabel='basis location',
\end_layout

\begin_layout Plain Layout

          ylabel='coefficient',
\end_layout

\begin_layout Plain Layout

          xlim=(0,10))
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Uso de 
\begin_inset Quotes fld
\end_inset

Ridge
\begin_inset Quotes frd
\end_inset

 en 
\begin_inset Quotes fld
\end_inset

Scikit-Learn
\begin_inset Quotes frd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
El modelo ajusta mucho mejor la onda senoidal, también podemos observar
 claramente como los coefientes varían para mejorar el ajuste.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename RidgeModeling.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Regresión Ridge
\end_layout

\end_inset

-
\end_layout

\end_inset


\end_layout

\begin_layout Section
Regressión Lasso o Regularización 
\begin_inset Formula $L_{1}$
\end_inset


\end_layout

\begin_layout Standard
Otra común regularización es conocidad como 
\begin_inset Quotes fld
\end_inset

Lasso
\begin_inset Quotes frd
\end_inset

, este se basa en penalizar la suma de los valores absolutos (1–norms) de
 la regresión de los coeficientes.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P=\alpha\cdot\sum_{n=1}^{N}\mid\theta_{n}\mid
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.linear_model import Lasso
\end_layout

\begin_layout Plain Layout

model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))
\end_layout

\begin_layout Plain Layout

basis_plot(model, title='Lasso Regression')
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Regressión Lasso
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Esta regresión tiende a favorecer modelos dispersos cuando sea posible:
 es decir, establece preferentemente los coeficientes del modelo exactamente
 en cero.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitSenoidalWithLasso.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Modelado de onda senoidal usando Lasso
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Support Vector Machines (SVMs)
\end_layout

\begin_layout Standard
Este tipo de modelado es particularmente poderoso y flexible algoritmos
 de clasificación supervisado.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.datasets.samples_generator import make_blobs
\end_layout

\begin_layout Plain Layout

X,y = make_blobs(n_samples=50, centers=2,
\end_layout

\begin_layout Plain Layout

                 random_state=0, cluster_std=0.60)
\end_layout

\begin_layout Plain Layout

plt.scatter( X[:,0], X[:,1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generando datos
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Un clasificador linear debería intentar dibujar una linea que separe los
 dos tipos de datos.
 Support vector machines ofrece una forma dibujar lineas que separen los
 datos, permitiendo clasificarlos por clases.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UseCaseOfSVMs.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVM para clasificación
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A continuación entrenaremos el SVM de Scikit-Learn con esos datos, notando
 que el paraemtro C=1E10 será explicado más adelante.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.svm import SVC
\end_layout

\begin_layout Plain Layout

model = SVC( kernel='linear', C=1E10 )
\end_layout

\begin_layout Plain Layout

model.fit(X,y)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Entrenando el modelo SVM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Para una mejor visualización, se agregara los limites de decisión del SVM,
 se implementa una función 
\begin_inset Quotes fld
\end_inset

plot_svc_decision_function
\begin_inset Quotes frd
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

def plot_svc_decision_function(model, ax=None, plot_support=True):
\end_layout

\begin_layout Plain Layout

    """Plot the decision function for a 2D SVC"""
\end_layout

\begin_layout Plain Layout

    if ax is None:
\end_layout

\begin_layout Plain Layout

        ax = plt.gca()
\end_layout

\begin_layout Plain Layout

    xlim = ax.get_xlim()
\end_layout

\begin_layout Plain Layout

    ylim = ax.get_ylim()
\end_layout

\begin_layout Plain Layout

    # create grid to evaluate model
\end_layout

\begin_layout Plain Layout

    x = np.linspace(xlim[0], xlim[1], 30)
\end_layout

\begin_layout Plain Layout

    y = np.linspace(ylim[0], ylim[1], 30)
\end_layout

\begin_layout Plain Layout

    Y, X = np.meshgrid(y, x)
\end_layout

\begin_layout Plain Layout

    xy = np.vstack([X.ravel(), Y.ravel()]).T
\end_layout

\begin_layout Plain Layout

    P = model.decision_function(xy).reshape(X.shape)
\end_layout

\begin_layout Plain Layout

    # plot decision boundary and margins
\end_layout

\begin_layout Plain Layout

    ax.contour(X, Y, P, colors='k',
\end_layout

\begin_layout Plain Layout

               levels=[-1, 0, 1], alpha=0.5,
\end_layout

\begin_layout Plain Layout

               linestyles=['--', '-', '--'])
\end_layout

\begin_layout Plain Layout

    # plot support vectors
\end_layout

\begin_layout Plain Layout

    if plot_support:
\end_layout

\begin_layout Plain Layout

        ax.scatter(model.support_vectors_[:, 0],
\end_layout

\begin_layout Plain Layout

                   model.support_vectors_[:, 1],
\end_layout

\begin_layout Plain Layout

                   s=300, linewidth=1, facecolors='none');
\end_layout

\begin_layout Plain Layout

    ax.set_xlim(xlim)
\end_layout

\begin_layout Plain Layout

    ax.set_ylim(ylim)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Implementación 
\begin_inset Quotes fld
\end_inset

plot_svc_decision_function
\begin_inset Quotes frd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Con tan solo dos líneas de código ahora podremos visualizar nuestros resultados:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

plot_svc_decision_function(model);
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visualización del modelo SVM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Los gráficos son los siguientes:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitModelSVM.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Limites del modelo SVM entrenado.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La validación del modelo se realizar con 60 y 120 puntos nuevos generados
 de forma aleatoria, todo esto definido en una función 
\begin_inset Quotes fld
\end_inset

plot_svm
\begin_inset Quotes frd
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

def plot_svm(N=10, ax=None):
\end_layout

\begin_layout Plain Layout

    X, y = make_blobs(n_samples=200, centers=2,
\end_layout

\begin_layout Plain Layout

                      random_state=0, cluster_std=0.60)
\end_layout

\begin_layout Plain Layout

    X = X[:N]
\end_layout

\begin_layout Plain Layout

    y = y[:N]
\end_layout

\begin_layout Plain Layout

    model = SVC(kernel='linear', C=1E10)
\end_layout

\begin_layout Plain Layout

    model.fit(X, y)
\end_layout

\begin_layout Plain Layout

    ax = ax or plt.gca()
\end_layout

\begin_layout Plain Layout

    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

    ax.set_xlim(-1, 4)
\end_layout

\begin_layout Plain Layout

    ax.set_ylim(-1, 6)
\end_layout

\begin_layout Plain Layout

    plot_svc_decision_function(model, ax)
\end_layout

\begin_layout Plain Layout

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
\end_layout

\begin_layout Plain Layout

fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
\end_layout

\begin_layout Plain Layout

for axi, N in zip(ax, [60, 120]):
\end_layout

\begin_layout Plain Layout

    plot_svm(N, axi)
\end_layout

\begin_layout Plain Layout

    axi.set_title('N = {0}'.format(N))
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validación del model SVM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
En los gráficos motrados se observa que el modelo SVM clasifica de forma
 muy aceptable los datos de validación generados.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ValidationModelSVM.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validación del modelo SVM
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Clasificador Kernel-SVM
\end_layout

\begin_layout Standard
Al mezclar SVM con Kernel, pudiendo ser 
\begin_inset Quotes fld
\end_inset

regresiones lineales
\begin_inset Quotes frd
\end_inset

, obtenemos un modelado muy poderoso, sobretodo para datos con elevada dimension
alidad.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python, style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.datasets.samples_generator import make_circles
\end_layout

\begin_layout Plain Layout

from sklearn.svm import SVC
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

import pandas as pd
\end_layout

\begin_layout Plain Layout

import matplotlib.pyplot as plt
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

X, y = make_circles(100, factor=.1, noise=.1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

clf = SVC(kernel='linear').fit(X,y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

plot_svc_decision_function(clf, plot_support=False)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Entrenando un modelo Kernel-SVM 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Tal como se observa en la visualización los datos no son linealmente separables.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitModelSVMwithLinealKernel.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clasificación de datos usando un 
\begin_inset Quotes fld
\end_inset

Kernel Lineal
\begin_inset Quotes frd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Agregando una dimensión adicional a los datos, clasificarlos se combierte
 en un problema lineal, para ello aplicaremos la transformación 
\begin_inset Formula $r=f\left(x\right)$
\end_inset

 definida como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r=e^{-x^{2}+1}
\]

\end_inset


\end_layout

\begin_layout Standard
Después de aplicar la transformación y agregar la nueva coordenada, vamos
 dibujar usando 3 dimensiones
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from ipywidgets import interact, fixed
\end_layout

\begin_layout Plain Layout

from mpl_toolkits import mplot3d
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

def plot_3D(X, y, z, elev=None, azim=None):
\end_layout

\begin_layout Plain Layout

    ax = plt.subplot(projection='3d')
\end_layout

\begin_layout Plain Layout

    ax.scatter3D( X[:,0], X[:,1], z, c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

    ax.view_init(elev=elev, azim=azim)
\end_layout

\begin_layout Plain Layout

    ax.set_xlabel('x')
\end_layout

\begin_layout Plain Layout

    ax.set_ylabel('y')
\end_layout

\begin_layout Plain Layout

    ax.set_zlabel('z')
\end_layout

\begin_layout Plain Layout

    plt.show()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

r = np.exp(-(X ** 2).sum(1))
\end_layout

\begin_layout Plain Layout

plot_3D( X, y, r)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Conversion de datos 2D a 3D.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La gráfica generada se muestra a continuación:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2Dto3DKernel-SVM.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visualización de los datos usando Kernel-SVM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
En esta estrategia, es computacionalmente costoso, ya que X tiene la forma
 
\begin_inset Formula $n\times n$
\end_inset

.
 Sin embargo, con un truco en el núcleo, es posible hacer un ajuste de los
 datos transformados por el núcleo de forma implícita, es decir, ¡sin countruir
 nunca l arepresentación N-dimensional completa de la proyección del núcleo!.
 Este truco esta integrado en el núcleo de SVM y es una de las razones por
 las que es poderoso.
\end_layout

\begin_layout Standard
En Scikit-Learn, podemos usar el 
\begin_inset Quotes fld
\end_inset

kernel-SVM
\begin_inset Quotes frd
\end_inset

 cambiando de 
\begin_inset Quotes fld
\end_inset

linear
\begin_inset Quotes frd
\end_inset

 a 
\begin_inset Quotes fld
\end_inset

rbf
\begin_inset Quotes frd
\end_inset

 (radial basis function):
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# Using a kernel Radial Basis Function
\end_layout

\begin_layout Plain Layout

clf = SVC(kernel='rbf', C=1E6)
\end_layout

\begin_layout Plain Layout

clf.fit(X,y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

plot_svc_decision_function(clf)
\end_layout

\begin_layout Plain Layout

plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1],
\end_layout

\begin_layout Plain Layout

            s=300, lw=1, facecolors='none')
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Usando 
\begin_inset Quotes fld
\end_inset

Radial Basis Fuction
\begin_inset Quotes frd
\end_inset

 Kernel-SVM 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Usando el 
\begin_inset Quotes fld
\end_inset

Kernel-SVM
\begin_inset Quotes frd
\end_inset

, podemos transformar datos en el mismo interior de los algoritmos para
 ejecutar metodos no lineales de forma más rápido, especialmente para modelos
 en los que se puede aplicar este 
\begin_inset Quotes fld
\end_inset

truco
\begin_inset Quotes frd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitmodelRDFKernel-SVM.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clasificación de datos usando 
\begin_inset Quotes fld
\end_inset

Kernel RDF-SVM
\begin_inset Quotes frd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Clasificador SVM con margenes suavizados
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

X, y = make_blobs(n_samples=100, centers=2,
\end_layout

\begin_layout Plain Layout

                  random_state=0, cluster_std=1.2)
\end_layout

\begin_layout Plain Layout

plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

plt.show()
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Datos solapados
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Tal como se observa a continuación los datos se solapan, esto podría hacer
 la clasificación más dificil.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DataSolapada.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visualización de datos solapados
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Para manejar este tipo de casos, la implementación de SVM tiene un 
\begin_inset Quotes fld
\end_inset

fudge-factor
\begin_inset Quotes frd
\end_inset

 que suavisa los margenes, la dureza de los margenes es ajustado por un
 parámetro C.
 Para un 
\begin_inset Quotes fld
\end_inset

C
\begin_inset Quotes frd
\end_inset

 muy grande, el margen es duro, y los puntos no pueden estar en él.
 Para un 
\begin_inset Quotes fld
\end_inset

C
\begin_inset Quotes frd
\end_inset

 más pequeño el margen es más suave y puede crecer hasta abarcar algunos
 puntos.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# Creating data: 100 samples with 2 centers
\end_layout

\begin_layout Plain Layout

X, y = make_blobs(n_samples=100, centers=2,
\end_layout

\begin_layout Plain Layout

                  random_state=0, cluster_std=0.8)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fig, ax = plt.subplots(1, 2, figsize=(16,6))
\end_layout

\begin_layout Plain Layout

fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for axi, C in zip(ax, [10.0, 0.1]):
\end_layout

\begin_layout Plain Layout

    model = SVC(kernel='linear', C=C).fit(X,y)
\end_layout

\begin_layout Plain Layout

    axi.scatter(X[:,0], X[:,1], c=y, s=50, cmap='autumn')
\end_layout

\begin_layout Plain Layout

    plot_svc_decision_function(model,axi)
\end_layout

\begin_layout Plain Layout

    axi.scatter(model.support_vectors_[:,0],
\end_layout

\begin_layout Plain Layout

                model.support_vectors_[:,1],
\end_layout

\begin_layout Plain Layout

                s=300, lw=1, facecolors='none')
\end_layout

\begin_layout Plain Layout

    axi.set_title('c = {0:.1f}'.format(C), size=14)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.show()
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clasificación de datos con diferentes valores de suavizado.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
El valor óptimo para el parametro 
\begin_inset Quotes fld
\end_inset

C
\begin_inset Quotes frd
\end_inset

 depende del dataset, por tanto debería ser ajustado usando 
\begin_inset Quotes fld
\end_inset

cross-validation
\begin_inset Quotes frd
\end_inset

 o un procedimiento similar.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitModelSVM-Softening.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVC con margenes suavizados (C=10.0 & C=0.1).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Face Recognition usando SVM
\end_layout

\begin_layout Standard
Usaremos rostros previamente clasificados, el cual consiste en miles de
 fotos de varias figuras públicas.
 Un buscador para estos datos esta integrada en Scikit-Learn:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

import matplotlib.pyplot as plt
\end_layout

\begin_layout Plain Layout

from sklearn.datasets import fetchs_lfw_people
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

faces = fetch_lfw_people(min_faces_per_person=60)
\end_layout

\begin_layout Plain Layout

print(faces.target_names)
\end_layout

\begin_layout Plain Layout

print(faces.images.shape)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fotos de rostros clasificados.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Esta base de datos cargada consta de rostros de personajes como: 
\begin_inset Quotes fld
\end_inset

Ariel Sharon
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

Colin Powell
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

Donald Rumsfeld
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

George W Bush
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

Gerald Schoroeder
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

Hugo Chavez
\begin_inset Quotes frd
\end_inset

, 
\begin_inset Quotes fld
\end_inset

Junichiro Koizumi
\begin_inset Quotes frd
\end_inset

 y 
\begin_inset Quotes fld
\end_inset

Tony Blair
\begin_inset Quotes frd
\end_inset

.
\end_layout

\begin_layout Standard
El tamaño de este dataset consta de 1348 muestras con imagenes de 
\begin_inset Quotes fld
\end_inset

62x47
\begin_inset Quotes frd
\end_inset


\end_layout

\begin_layout Standard
A continuación se mostrará algunas imagenes de los rostros del dataset cargado.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

fig, ax = plt.subplots(3,5)
\end_layout

\begin_layout Plain Layout

for i, axi in enumerate(ax.flat):
\end_layout

\begin_layout Plain Layout

    axi.imshow(faces.images[i], cmap='bone')
\end_layout

\begin_layout Plain Layout

    axi.set(xticks=[], yticks=[],
\end_layout

\begin_layout Plain Layout

            xlabel=faces.target_names[faces.target[i]])
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Mostrando algunos rostros
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DataSetFaces.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Algunos rostros del dataset.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cada imagen contiene [62x47] cerca de 3000 Pixeles.
 Podríamos considerar cada valor del pixel como una característica pero
 a menudo es más efectivo usar algun preprocesador para extraer las característi
cas más significativas, aquí usaremos el 
\begin_inset Quotes fld
\end_inset

Análisis de Componentes Principales
\begin_inset Quotes frd
\end_inset

 para extraer 150 componentes principales e introducirlas al clasificador
 SVM.
 Podemos hacer esto de manera directa empaquetando el procesador y clasificador
 en un único 
\begin_inset Quotes fld
\end_inset

pipeline
\begin_inset Quotes frd
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.svm import SVC
\end_layout

\begin_layout Plain Layout

from sklearn.decomposition import PCA as RandomizedPCA
\end_layout

\begin_layout Plain Layout

from sklearn.pipeline import make_pipeline
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)
\end_layout

\begin_layout Plain Layout

svc = SVC(kernel='rbf', class_weight='balanced')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

model = make_pipeline(pca,svc)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Empaquetando PCA y SVM en un pipeline
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Para validar nuestro modelo, dividiremos los datos en 2 conjuntos: 
\begin_inset Quotes fld
\end_inset

entrenamiento
\begin_inset Quotes frd
\end_inset

 y 
\begin_inset Quotes fld
\end_inset

prueba
\begin_inset Quotes frd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

#Deprecated: cross_validation by model_selection
\end_layout

\begin_layout Plain Layout

from sklearn.cross_validation import train_test_split
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data,
\end_layout

\begin_layout Plain Layout

                                                faces.target,
\end_layout

\begin_layout Plain Layout

                                                random_state=42)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Creando los conjuntos de datos 
\begin_inset Quotes fld
\end_inset

entrenamiento
\begin_inset Quotes frd
\end_inset

 y 
\begin_inset Quotes fld
\end_inset

prueba
\begin_inset Quotes frd
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finalmente usaremos un 
\begin_inset Quotes fld
\end_inset

grid search cross-validation
\begin_inset Quotes frd
\end_inset

 para explorar las combinaciones de parametros.
 Aqui ajustaremos 
\begin_inset Quotes fld
\end_inset

C
\begin_inset Quotes frd
\end_inset

 (controlar la dureza de los margenes) y 
\begin_inset Quotes fld
\end_inset

gamma
\begin_inset Quotes frd
\end_inset

 (controlar el tamaño del núcleo de la funcion de base radial)
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.model_selection import GridSearchCV
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

param_grid = { 'svc__C'     : [    1,     5,   10,   50],
\end_layout

\begin_layout Plain Layout

               'svc__gamma' : [.0001, .0005, .001, .005]}
\end_layout

\begin_layout Plain Layout

grid = GridSearchCV(model, param_grid)
\end_layout

\begin_layout Plain Layout

grid.fit(Xtrain, ytrain)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

print(grid.best_params_)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Training model using grid search cross-validation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Los mejores valores encontrados para los parametros son : 
\begin_inset Formula ${'\text{svc\_\_c}':10,'\text{svc\_\_gamma}':0.001}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Ahora usaremos este model para predecir valores con los datos de prueba.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

model = grid.best_estimator_
\end_layout

\begin_layout Plain Layout

yfit = model.predict(Xtest)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validando el modelo 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Podemos obtener una mejor visión de nuestro clasificador generando un reporte,
 tal como se muestra a continuación:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.metrics import classification_report
\end_layout

\begin_layout Plain Layout

print(classification_report(ytest, yfit,
\end_layout

\begin_layout Plain Layout

                            target_names=faces.target_names))
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Reporte de clasificación
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Según se observa en el reporte, podemos 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FitModelReport-PCA-SVM.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Reporte de clasificación del modelo 
\begin_inset Quotes fld
\end_inset

PCA-SVM
\begin_inset Quotes frd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
También podemos mostrar la matriz de confusión, para tener una mejor visualizaci
ón de nuestro modelo.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.metrics import confusion_matrix
\end_layout

\begin_layout Plain Layout

import seaborn as sns
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

mat = confusion_matrix(ytest, yfit)
\end_layout

\begin_layout Plain Layout

sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
\end_layout

\begin_layout Plain Layout

            xticklabels=faces.target_names,
\end_layout

\begin_layout Plain Layout

            yticklabels=faces.target_names)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.xlabel('true label')
\end_layout

\begin_layout Plain Layout

plt.ylabel('predicted label')
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Matriz de Confusión
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename MatrixConfussionModelFitFaces.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Matriz de confusión del clasificador de rostros.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Para este tipo de aplicaciones, es mejor usar OpenCV, que tiene implementado
 modelos para extraer caracteristicas de las imagenes independiente de la
 pixelación.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Este metodo es muy poderoso para clasificar por las siguientes razones:
\end_layout

\begin_layout Itemize
Una vez el modelo fue entrenado, la fase de la predición es rápida.
\end_layout

\begin_layout Itemize
Debido a que solo se ven afectados por puntos cercanos al margen, estos
 trabajan bien con datos hiperdimensionales, incluso con datos que tienes
 más dimensiones que muestras, que muchas veces es un reto para otros algoritmos.
\end_layout

\begin_layout Itemize
Su integración con 
\begin_inset Quotes fld
\end_inset

kernels
\begin_inset Quotes frd
\end_inset

 hace que sea muy versatil y adaptable a muchos tipos de datos.
\end_layout

\begin_layout Plain Layout
Sin embargo SVM tiene desventajas:
\end_layout

\begin_layout Itemize
Para muestras de volumen N es 
\begin_inset Formula $O\left[N^{3}\right]$
\end_inset

 en el peor caso, o 
\begin_inset Formula $O\left[N^{2}\right]$
\end_inset

 para implementaciones eficientes.
 Para un gran número de muestras de entrenamiento, esto puede ser computacionalm
ente prohibitivo.
\end_layout

\begin_layout Itemize
Los resultados dependen grandemente de la elección adecuada del parametro
 de suavizado 
\begin_inset Quotes fld
\end_inset

C
\begin_inset Quotes frd
\end_inset

.
 Este debe elegirse cuidadosamente mediante validación cruzada, lo que puede
 ser costoso a medida que los datos crecen en tamaño.
\end_layout

\begin_layout Itemize
Los resultados no tienen una interpretación probabilística directa.
 Esto se puede estimar a trav+es de una validación cruzada interna (ver
 el parámetro de probabilidad SVC), pero esta estimación adicional es costosa.
\end_layout

\begin_layout Plain Layout
Con estos rasgos en mente, únicamente se debe recurrir a SVM, después que
 otros métodos más simples, más rápidos y menos intensivos de ajuste han
 demostrado ser insuficientes.
 Sin embargo, si tienes 
\series bold
ciclos de CPU
\series default
 para entrenamiento y la validación cruzada de una SVM, el metodo puede
 generar excelentes resultados.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Clasificador Naive Bayes
\end_layout

\begin_layout Standard
Cargando los datos del 
\begin_inset Quotes eld
\end_inset

iris
\begin_inset Quotes erd
\end_inset

 (n_sampes x n_features)
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

import seaborn as sns
\end_layout

\begin_layout Plain Layout

iris = sns.load_dataset('iris')
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cargando datos 
\begin_inset Quotes eld
\end_inset

iris
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Una visualización parcial del dataframe se puede realizar ejecutando el
 comando 
\begin_inset Quotes eld
\end_inset

iris.head()
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sepal_length
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sepal_width
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
petal_length
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
petal_width
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
species
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setosa
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setosa
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setosa
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setosa
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0,2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setosa
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
iris.head()
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La matriz de características será 
\begin_inset Quotes eld
\end_inset

X
\begin_inset Quotes erd
\end_inset

 y el objetivo o target será la columna 
\begin_inset Quotes eld
\end_inset

species
\begin_inset Quotes erd
\end_inset

, el cual puede tomar 3 valores: 
\begin_inset Quotes eld
\end_inset

setosa
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

versicolor
\begin_inset Quotes erd
\end_inset

 y 
\begin_inset Quotes eld
\end_inset

virginica
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# Eliminamos la columna 'species' del dataframe
\end_layout

\begin_layout Plain Layout

X_iris = iris.drop('species', axis=1)
\end_layout

\begin_layout Plain Layout

# Almacenamos el target 
\begin_inset Quotes eld
\end_inset

species
\begin_inset Quotes erd
\end_inset

 en la variable 
\begin_inset Quotes eld
\end_inset

y_iris
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout

y_iris = iris['species']
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

features
\begin_inset Quotes erd
\end_inset

 & 
\begin_inset Quotes eld
\end_inset

target
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Se procederá a dividir los datos en 2 tipos:
\end_layout

\begin_layout Itemize
Training set: datos para el entrenamiento del modelo
\end_layout

\begin_layout Itemize
Testing set: datos para probar el modelo entrenado
\end_layout

\begin_layout Standard
Este tipo de partición de los datos puede ser manual, pero es mas conveniente
 usar 
\begin_inset Quotes eld
\end_inset

train_test_split
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

#Deprecado: from sklearn.cross_validation import train_test_split
\end_layout

\begin_layout Plain Layout

from sklearn.model_selection import train_test_split
\end_layout

\begin_layout Plain Layout

Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1,
 test_size=0.33)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Dividiendo los datos en entrenamiento y prueba
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Para hacer la clasificación se va a realizar un modelo conocido como Naive
 Bayes Gaussiano, asumiendo que cada clase parte de una distribución Gaussiana.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# 1.Choose model class
\end_layout

\begin_layout Plain Layout

from sklearn.naive_bayes import GaussianNB
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 2.Instantiate model
\end_layout

\begin_layout Plain Layout

model = GaussianNB()
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 3.Fit model to data
\end_layout

\begin_layout Plain Layout

model.fit(Xtrain, ytrain)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 4.Predict on new data
\end_layout

\begin_layout Plain Layout

y_model = model.predict(Xtest)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Entrenando el modelo Gaussiano
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finalmente, podemos usar 
\begin_inset Quotes eld
\end_inset

accuracy_score
\begin_inset Quotes erd
\end_inset

 para ver la proporción predicha con respecto al valor verdadero.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.metrics import accuracy_score
\end_layout

\begin_layout Plain Layout

accuracy_score(ytest, ymodel)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
accuracy_score
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Nuestro modelo tiene un 97.36% de precisión, como se puede observar el algoritmo
 de clasificación naive es efectivo para este caso particular de datos.
\end_layout

\begin_layout Section
Reducción de componentes usando PCA
\end_layout

\begin_layout Standard
Un ejemplo del problema del aprendizaje no supervisado es la reducción de
 la dimensionalidad, para hacer más facil su visualización.
 Usando los datos 
\begin_inset Quotes eld
\end_inset

iris
\begin_inset Quotes erd
\end_inset

, vamos extraer las características escenciales de la data.
 A menudo, la reducción dimensional es usada para visualizar los datos de
 forma más fácil en 2 dimensiones.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# 1.Choose the model class
\end_layout

\begin_layout Plain Layout

from sklearn.decomposition import PCA
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 2.Instantiate the model with hyperparameters
\end_layout

\begin_layout Plain Layout

model = PCA(n_components=2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 3.Fit to data.
 Notice 'y' is not specified!
\end_layout

\begin_layout Plain Layout

model.fit(X_iris)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 4.Transform the data to two dimensions
\end_layout

\begin_layout Plain Layout

X_2D = model.transform(X_iris)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Análisis de Componentes Principales con los datos de 
\begin_inset Quotes eld
\end_inset

iris
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Los datos de 
\begin_inset Quotes eld
\end_inset

X_iris
\begin_inset Quotes erd
\end_inset

 tiene 4 caracteristicas, las cuales fueron reducidas a 2 componentes en
 
\begin_inset Quotes eld
\end_inset

X_2D
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

iris['PCA1'] = X_2D[:,0]
\end_layout

\begin_layout Plain Layout

iris['PCA2'] = X_2D[:,1]
\end_layout

\begin_layout Plain Layout

sns.lmplot('PCA1','PCA2', hue='species', data=iris, fit_reg=False)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Agregando PCA1 y PCA2 al dataframe 
\begin_inset Quotes eld
\end_inset

iris.
\end_layout

\end_inset


\end_layout

\end_inset

A continuación se muestran los nuevos datos en una representación bidimensional.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ReductionPCA.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
PCA1 y PCA2 de los datos 
\begin_inset Quotes eld
\end_inset

iris
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Clustering con Gaussian Mixture Model (GMM)
\end_layout

\begin_layout Standard
A continuación se va a clasificar los datos usando un poderoso método llamado
 Gaussian Mixture Model (GMM).
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

# 1.Choose the model class
\end_layout

\begin_layout Plain Layout

# Deprecated : from sklearn.mixture import GMM
\end_layout

\begin_layout Plain Layout

from sklearn.mixture import GaussianMixture
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 2.Instantiate the model with hyperparameters
\end_layout

\begin_layout Plain Layout

model = GMM(n_components=3, covariance_type='full')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 3.Fit to data.
 Notice 'y' is not specified!
\end_layout

\begin_layout Plain Layout

model.fit(X_iris)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# 4.Determine cluster labels
\end_layout

\begin_layout Plain Layout

y_gmm = model.predict(X_iris)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clustering con GMM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cargando la columna de datos predichos al dataframe 'iris'
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

iris['cluster'] = y_gmm
\end_layout

\begin_layout Plain Layout

sns.lmplot('PCA1', 'PCA2', data=iris, hue='species',
\end_layout

\begin_layout Plain Layout

col='cluster', fit_reg=False)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Graficando los datos clasificados con GMM
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
A continuación veremos los datos clasificados pero para tener una mejor
 visualización usaremos PCA1 y PCA2, calculados anteriormente.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename GMM.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
PCA1 y PCA2 usando GMM
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Como se puede observar en el 
\begin_inset Quotes eld
\end_inset

cluster 0
\begin_inset Quotes erd
\end_inset

, vemos 5 puntos de color naranja que en comparación con lo demás en general
 es un pequeño error aceptable del modelo entrenado.
\end_layout

\begin_layout Section
Árboles de Decisión y Bosques Aleatorios
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

import matplotlib.pyplot as plt
\end_layout

\begin_layout Plain Layout

import seaborn as sns
\end_layout

\begin_layout Plain Layout

from sklearn.datasets import make_blobs
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

sns.set()
\end_layout

\begin_layout Plain Layout

X, y = make_blobs(n_samples=300,  centers=4,
\end_layout

\begin_layout Plain Layout

                  random_state=0, cluster_std=1.0)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='rainbow')
\end_layout

\begin_layout Plain Layout

plt.show()
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Datasets con 4 tipos.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DataSetRandomForest.png
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Dataset para clasificar con 
\begin_inset Quotes fld
\end_inset

Random Forest
\begin_inset Quotes frd
\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Un árbol simple de decisión interactúa con los datos dividiendolos usando
 ejes, acorde a algún criterio cuantitativo 
\begin_inset Quotes fld
\end_inset

depth
\begin_inset Quotes frd
\end_inset

.
 La figura acontinuación representa para los primeros cuatro niveles de
 un árbol de clasificación:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename DecisionTreeLevels.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Visualización para los diferentes niveles del árbol de decisión.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

#from sklearn.datasets.samples_generator import make_circles 
\end_layout

\begin_layout Plain Layout

#from sklearn.svm import SVC 
\end_layout

\begin_layout Plain Layout

from sklearn.datasets.samples_generator import make_blobs 
\end_layout

\begin_layout Plain Layout

from sklearn.tree import DecisionTreeClassifier 
\end_layout

\begin_layout Plain Layout

from ipywidgets import interact 
\end_layout

\begin_layout Plain Layout

import matplotlib.pyplot as plt 
\end_layout

\begin_layout Plain Layout

import numpy as np 
\end_layout

\begin_layout Plain Layout

import pandas as pd ############################################################
############################ 
\end_layout

\begin_layout Plain Layout

def visualize_tree(estimator, X, y, boundaries=True,
\end_layout

\begin_layout Plain Layout

                   xlim=None, ylim=None, axi=None):
\end_layout

\begin_layout Plain Layout

    ax = axi or plt.gca()
\end_layout

\begin_layout Plain Layout

    # Plot the training points     
\end_layout

\begin_layout Plain Layout

    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',
\end_layout

\begin_layout Plain Layout

               clim=(y.min(), y.max()), zorder=3)
\end_layout

\begin_layout Plain Layout

    ax.axis('tight')
\end_layout

\begin_layout Plain Layout

    ax.axis('off')
\end_layout

\begin_layout Plain Layout

    if xlim is None:
\end_layout

\begin_layout Plain Layout

        xlim = ax.get_xlim()
\end_layout

\begin_layout Plain Layout

    if ylim is None:
\end_layout

\begin_layout Plain Layout

        ylim = ax.get_ylim()
\end_layout

\begin_layout Plain Layout

         # fit the estimator
\end_layout

\begin_layout Plain Layout

    estimator.fit(X, y)
\end_layout

\begin_layout Plain Layout

    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
\end_layout

\begin_layout Plain Layout

                         np.linspace(*ylim, num=200))
\end_layout

\begin_layout Plain Layout

    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])
\end_layout

\begin_layout Plain Layout

    # Put the result into a color plot
\end_layout

\begin_layout Plain Layout

    n_classes = len(np.unique(y))
\end_layout

\begin_layout Plain Layout

    Z = Z.reshape(xx.shape)
\end_layout

\begin_layout Plain Layout

    contours = ax.contourf(xx, yy, Z, alpha=0.3,
\end_layout

\begin_layout Plain Layout

                           levels=np.arange(n_classes + 1) - 0.5,
\end_layout

\begin_layout Plain Layout

                           cmap='viridis', clim=(y.min(), y.max()),
\end_layout

\begin_layout Plain Layout

                           zorder=1)
\end_layout

\begin_layout Plain Layout

    ax.set(xlim=xlim, ylim=ylim)
\end_layout

\begin_layout Plain Layout

    # Plot the decision boundaries
\end_layout

\begin_layout Plain Layout

    def plot_boundaries(i, xlim, ylim):
\end_layout

\begin_layout Plain Layout

        if i >= 0:
\end_layout

\begin_layout Plain Layout

            tree = estimator.tree_
\end_layout

\begin_layout Plain Layout

                     if tree.feature[i] == 0:
\end_layout

\begin_layout Plain Layout

                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k',
 zorder=2)
\end_layout

\begin_layout Plain Layout

                plot_boundaries(tree.children_left[i],                  
               [xlim[0], tree.threshold[i]], ylim)                 plot_boundarie
s(tree.children_right[i],                                 [tree.threshold[i],
 xlim[1]], ylim)                      elif tree.feature[i] == 1:        
         ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)
                 plot_boundaries(tree.children_left[i], xlim,           
                      [ylim[0], tree.threshold[i]])                 plot_boundari
es(tree.children_right[i], xlim,                                 [tree.threshold[i
], ylim[1]])                  if boundaries:         plot_boundaries(0,
 xlim, ylim) ###################################################################
##################### def plot_tree_interactive(X, y):     def interactive_tree(
depth=5):         clf = DecisionTreeClassifier(max_depth=depth, random_state=0)
         visualize_tree(clf, X, y)     #return interact(interactive_tree,
 depth=[1,5])     return interactive_tree(depth=[1,5]) #########################
############################################################### def randomized_t
ree_interactive(X, y):     N = int(0.75 * X.shape[0])          xlim = (X[:,
 0].min(), X[:, 0].max())     ylim = (X[:, 1].min(), X[:, 1].max())        
  def fit_randomized_tree(random_state=0):         clf = DecisionTreeClassifier(
max_depth=15)         i = np.arange(len(y))         rng = np.random.RandomState(ran
dom_state)         rng.shuffle(i)         visualize_tree(clf, X[i[:N]], y[i[:N]],
 boundaries=False,                        xlim=xlim, ylim=ylim)        
  interact(fit_randomized_tree, random_state=[0, 100]); ########################
################################################################ # Creating
 dataset X, y = make_blobs(n_samples=300, centers=4,                   random_st
ate=0, cluster_std=1.0)
\end_layout

\begin_layout Plain Layout

# Creating subplots 1x4 fig, ax = plt.subplots( 1, 4, figsize=(16,3) ) fig.subplot
s_adjust(left=0.02, right=0.98, wspace=0.1)
\end_layout

\begin_layout Plain Layout

# Fiting and plotting for axi, depth in zip(ax, range(1,5)):     model =
 DecisionTreeClassifier(max_depth=depth)     visualize_tree(model, X, y,
 axi=axi)     axi.set_title('depth = {0}'.format(depth))
\end_layout

\begin_layout Plain Layout

# Show figure: Decision-tree-levels plt.show() 
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tree-
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Part
Hiperparametros y Validación de Modelos
\end_layout

\begin_layout Section
Validar un modelo usando validación cruzada
\end_layout

\begin_layout Standard
Para realizar la validación de forma rápida se utilizare 
\begin_inset Quotes eld
\end_inset

train_test_split
\begin_inset Quotes erd
\end_inset

, con el parámetro train_size=0.5 los datos seran divididos en 50% para entrenami
ento y 50% para la validación.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

#Deprecated: cross_validation by model_selection
\end_layout

\begin_layout Plain Layout

from sklearn.model_selection import train_test_split
\end_layout

\begin_layout Plain Layout

from sklearn.metrics import accuracy_score
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# split the data with 50% in each set
\end_layout

\begin_layout Plain Layout

X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# fit and evalue the model
\end_layout

\begin_layout Plain Layout

y2_model = model.fit(X1, y1).predict(X2)
\end_layout

\begin_layout Plain Layout

y1_model = model.fit(X2, y2).predict(X1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# compute the accuracy by every group
\end_layout

\begin_layout Plain Layout

accuracy_score(y1,y1_model), accuracy_score(y2, y2_model)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validación cruzada usando 
\begin_inset Quotes eld
\end_inset

train_test_split
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Podemos expandir para una validación cruzada y usarla para más pruebas,
 por ejemplo separar en 5 grupos (cv=5) y validar cada grupom para ello
 se usara 
\begin_inset Quotes eld
\end_inset

cross_val_score
\begin_inset Quotes erd
\end_inset

, se entrenara el model con 4/5 y se evaluara el modelo con el 1/5 restante.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.cross_validation import cross_val_score
\end_layout

\begin_layout Plain Layout

cross_vol_score(model, X, y, cv=5)
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validación cruzada de 5 grupos usando 
\begin_inset Quotes eld
\end_inset

cross_val_score
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La respuesta es una <numpy.array> por ejemplo: array([0.96666667, 0.96666667,
 0.93333333, 1.
 ])
\end_layout

\begin_layout Standard
También podemos hacer una validación 
\begin_inset Quotes eld
\end_inset

uno contra todos
\begin_inset Quotes erd
\end_inset

, dejamos unicamente un dato para realizar la validación, para ello se usa
 
\begin_inset Quotes eld
\end_inset

LeaveOneOut
\begin_inset Quotes erd
\end_inset

, tal como se muestra a continuación:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,style=mystyle]
\end_layout

\begin_layout Plain Layout

from sklearn.cross_validation import LeaveOneOut
\end_layout

\begin_layout Plain Layout

scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
\end_layout

\begin_layout Plain Layout

scores_mean = scores.mean()
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Validación cruzada uno contra todos usando 
\begin_inset Quotes eld
\end_inset

LeaveOneOut
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Validación de curvas
\end_layout

\end_body
\end_document
